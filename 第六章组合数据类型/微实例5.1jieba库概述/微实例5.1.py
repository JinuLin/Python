# 微实例6.5.1jieba库概述
import jieba
# 对于一段英文文本，我们希望得到其中的单词，只需要所以字符串处理的split()方法即可
print("China is a great country".split())
"""
对于中文文本，我们希望得到其中的词语，需要借助jieba分词库，
jieba是第三方库，需要先通过pip安装：pip install jieba
"""
jieba.lcut("中国是一个伟大的国家")
"""
jieba的分词库原理是利用一个词库，将文本中的词语进行匹配，
通过图结构和动态规划算法，找到最优的分词方式
jieba还提供增加自定义词典的功能
"""
"""
jieba库支持3种分词模式：
精确模式、全模式、搜索引擎模式
精确模式：将句子最精确地切开，适合文本分析，默认模式
全模式：把句子中所有的可以切分的词语都切分出来，速度非常快，但是不能消除歧义
搜索引擎模式：在精确模式基础上，对长词再次切分，提高召回率，适合搜索引擎分词
"""

