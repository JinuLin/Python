# 微实例9.1.1requests库概述
"""
Robots排除协议（Robots Exclusion Protocol），也被称为爬虫协议，
它是网站管理者表达是否希望爬虫自动获取网络信息意愿的方法。
管理者可以在网站根目录放置一个robots.txt文件，
并在文件中列出哪些链接不允许爬虫爬取。
一般搜索引擎的爬虫会首先捕获这个文件，并根据文件要求爬取网站内容。
Robots排除协议重点约定不希望爬虫获取的内容，
如果没有该文件则表示网站内容可以被爬虫获得，然而，Robots协议不是命令和强制手段，
只是国际互联网的一种通用道德规范，绝大部分成熟的搜索引擎爬虫都会遵循这个协议。
"""
import requests
"""

"""
print(requests.__version__) # 2.18.1
