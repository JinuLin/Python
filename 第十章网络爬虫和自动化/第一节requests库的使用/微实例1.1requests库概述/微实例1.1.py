# 微实例10.1.1requests库概述
"""
Robots排除协议（Robots Exclusion Protocol），也被称为爬虫协议，
它是网站管理者表达是否希望爬虫自动获取网络信息意愿的方法。
管理者可以在网站根目录放置一个robots.txt文件，
并在文件中列出哪些链接不允许爬虫爬取。
一般搜索引擎的爬虫会首先捕获这个文件，并根据文件要求爬取网站内容。
Robots排除协议重点约定不希望爬虫获取的内容，
如果没有该文件则表示网站内容可以被爬虫获得，然而，Robots协议不是命令和强制手段，
只是国际互联网的一种通用道德规范，绝大部分成熟的搜索引擎爬虫都会遵循这个协议。
"""
import requests
"""
requests库是一个简洁且简单的处理HTTP请求的第三方库。
requests的最大优点是程序编写过程更接近正常URL访问过程。
这个库建立在Python语言的urllib3库基础上，
类似这种在其他函数库之上再封装功能提供更友好函数的方式在Python语言中十分常见。
request库支持非常丰富的链接访问功能，
包括：国际域名和URL获取、HTTP长连接和连接缓存、HTTP会话和Cookie保持、
浏览器使用风格的SSL验证、基本的摘要认证、有效的键值对Cookie记录、
自动解压缩、自动内容解码、文件分块上传、HTTP(S)代理功能、连接超时处理、流数据下载等。
"""
print(requests.__version__) # 2.18.1
