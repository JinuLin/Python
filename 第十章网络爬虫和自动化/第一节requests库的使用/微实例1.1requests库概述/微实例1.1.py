# 微实例9.1.1requests库概述
"""
Robots排除协议（Robots Exclusion Protocol），也被称为爬虫协议，
它是网站管理者表达是否希望爬虫自动获取网络信息意愿的方法。
管理者可以在网站根目录放置一个robots.txt文件，
并在文件中列出哪些链接不允许爬虫爬取。
一般搜索引擎的爬虫会首先捕获这个文件，并根据文件要求爬取网站内容。
Robots排除协议重点约定不希望爬虫获取的内容，
如果没有该文件则表示网站内容可以被爬虫获得，然而，Robots协议不是命令和强制手段，
只是国际互联网的一种通用道德规范，绝大部分成熟的搜索引擎爬虫都会遵循这个协议。
"""
import requests
"""
requests库是一个简洁且简单的处理HTTP请求的第三方库。
requests的最大优点是程序编写过程更接近正常URL访问过程。
这个库建立在Python 语言的urllib3 库基础上，类
似这种在其他函数库之上再封装功能提供更友好函数
的方式在Python 语言中十分常见。在Python 的生
态圈里，任何人都有通过技术创新或体验创新发表意
见和展示才华的机会。
"""
print(requests.__version__) # 2.18.1
